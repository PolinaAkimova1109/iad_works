{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IHgmxWG_7lnE"
   },
   "source": [
    "# Введение в анализ данных\n",
    "## НИУ ВШЭ, 2019-2020 учебный год\n",
    "\n",
    "### Домашнее задание №3\n",
    "\n",
    "Задание выполнил(а): Акимова Полина\n",
    "\n",
    "### Общая информация\n",
    "Дата выдачи: \n",
    "\n",
    "Дедлайн: \n",
    "\n",
    "### О задании\n",
    "\n",
    "В этом домашнем задании вы будете работать с линейной классификацией, попрактикуетесь на реальной задаче классификации текстов.\n",
    "\n",
    "Для решения этого домашнего задания намного удобнее будет использовать Colab, так как данных много.\n",
    "\n",
    "### Оценивание и штрафы\n",
    "\n",
    "За сдачу задания позже срока на итоговую оценку за задание накладывается штраф в размере 1 балл в день, но получить отрицательную оценку нельзя.\n",
    "\n",
    "__Внимание!__ Домашнее задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов.\n",
    "\n",
    "### Формат сдачи\n",
    "Загрузка файлов с решениями происходит в системе Anytask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ztx03xvr9T95"
   },
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BVrrwTJNjuDt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "id": "_VMchexbjjTh",
    "outputId": "122b1d4b-da19-44b3-87fa-d32398d73a54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-04-26 11:33:50--  https://www.dropbox.com/s/tg55q9mrziroyrs/train_subset.csv\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.82.1, 2620:100:6032:1::a27d:5201\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.82.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/raw/tg55q9mrziroyrs/train_subset.csv [following]\n",
      "--2020-04-26 11:33:51--  https://www.dropbox.com/s/raw/tg55q9mrziroyrs/train_subset.csv\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc84df927baf2213129e0369f21d.dl.dropboxusercontent.com/cd/0/inline/A2kOkwONc8wGTgDk36BRN8qoedSphc7ZoqzZNbkWAI5Z-psUvgT-KWid8snRsSy1SwRstcFACDEuI5vk0nZE50eAdGx5jYvn8Pid7yQxdBFMXN3Bkr0zeMezWd-91LqlHUU/file# [following]\n",
      "--2020-04-26 11:33:51--  https://uc84df927baf2213129e0369f21d.dl.dropboxusercontent.com/cd/0/inline/A2kOkwONc8wGTgDk36BRN8qoedSphc7ZoqzZNbkWAI5Z-psUvgT-KWid8snRsSy1SwRstcFACDEuI5vk0nZE50eAdGx5jYvn8Pid7yQxdBFMXN3Bkr0zeMezWd-91LqlHUU/file\n",
      "Resolving uc84df927baf2213129e0369f21d.dl.dropboxusercontent.com (uc84df927baf2213129e0369f21d.dl.dropboxusercontent.com)... 162.125.82.6, 2620:100:6032:6::a27d:5206\n",
      "Connecting to uc84df927baf2213129e0369f21d.dl.dropboxusercontent.com (uc84df927baf2213129e0369f21d.dl.dropboxusercontent.com)|162.125.82.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19213119 (18M) [text/plain]\n",
      "Saving to: ‘train_subset.csv.1’\n",
      "\n",
      "train_subset.csv.1  100%[===================>]  18.32M  20.9MB/s    in 0.9s    \n",
      "\n",
      "2020-04-26 11:33:53 (20.9 MB/s) - ‘train_subset.csv.1’ saved [19213119/19213119]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Датасет можно скачать здесь\n",
    "\n",
    "!wget https://www.dropbox.com/s/tg55q9mrziroyrs/train_subset.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rvXKae8q9nn-"
   },
   "source": [
    "### Данные\n",
    "\n",
    "Мы имеем дело с данными с торговой платформы Avito.\n",
    "Для каждого товара представлены следующие параметры:\n",
    " - title\n",
    " - description\n",
    " - Category_name\n",
    " - Category\n",
    "\n",
    "Имеется информация об объектах 50 классов.\n",
    "Задача: по новым объектам (title, description) предсказать Category.\n",
    "(Очевидно, что параметр Category_name для предсказания классов использовать нельзя)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "BqEuoDhqNgoa",
    "outputId": "ea368895-17a8-4e2f-a48f-e20ee176ecef"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>Category_name</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>382220</th>\n",
       "      <td>Прихожая</td>\n",
       "      <td>В хорошем состоянии. Торг</td>\n",
       "      <td>Мебель и интерьер</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397529</th>\n",
       "      <td>Кордиант 215/55/16 Летние</td>\n",
       "      <td>Кордиант 215/55/16 Летние/\\n /\\nАртикул: 1737l...</td>\n",
       "      <td>Запчасти и аксессуары</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584569</th>\n",
       "      <td>Стол</td>\n",
       "      <td>Стол, 2 рабочих места . Стол серого цвета, в д...</td>\n",
       "      <td>Мебель и интерьер</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513100</th>\n",
       "      <td>Комбинезон</td>\n",
       "      <td>Размер-42/44</td>\n",
       "      <td>Одежда, обувь, аксессуары</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091886</th>\n",
       "      <td>Ветровка</td>\n",
       "      <td>На 2 года</td>\n",
       "      <td>Детская одежда и обувь</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title  ... Category\n",
       "id                                  ...         \n",
       "382220                    Прихожая  ...       20\n",
       "397529   Кордиант 215/55/16 Летние  ...       10\n",
       "584569                        Стол  ...       20\n",
       "2513100                 Комбинезон  ...       27\n",
       "1091886                   Ветровка  ...       29\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"train_subset.csv\", index_col='id')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Kg8iPp7fiwGh",
    "outputId": "b8165d94-050b-459c-b35c-f0bc86c3f2b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A1hvzAMETU2d"
   },
   "outputs": [],
   "source": [
    "X = data[['title', 'description']].to_numpy()\n",
    "y = data['Category'].to_numpy()\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tMYU7zZw_cw-"
   },
   "source": [
    "Сразу разделим выборку на train и test.\n",
    "Никакие данные из test для обучения использовать нельзя!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6fia4_3vNprp"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "qDR8LtTJUIGt",
    "outputId": "530f8e9d-342d-4101-ad5a-f27d7648e380"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Сапоги 46 размер новые', 'Сапоги 46 размер новые'],\n",
       "       ['Светильники потолочный swarovski',\n",
       "        'светильники потолочные swarovski 6 штук , цена за штуку. В эксплуатации 2 года , продаются в связи со сменой интерьера в квартире'],\n",
       "       ['iPhone 7 plus 128GB Red красный в наличии',\n",
       "        '\\xa0/\\n/\\n Данная цена только для подписчиков Instagram: iQmac/\\n/\\n Новый красный айфон 7 Plus в наличии это элегантный и мощный смартфон, который готов в полной мере раскрыть новые возможности iOS 10. Аппарат с 4-ядерным процессором А10 и 3 ГБ ОЗУ с легкостью решает самые ресурсоемкие задачи, позволяя наслаждаться быстродействием «тяжелых» приложений и игр на 5,5-дюймовом дисплее. Аппарат получил экран, как у iPad Pro, так что картинка теперь соответствует кинематографическому стандарту.'],\n",
       "       ['Пион Ирис Ромашка рассада',\n",
       "        'Пион куст 500 р ( более 10 шт)/\\nСаженец/ корень 100р/\\nРастут у нас более 70 лет/\\nРозовые, бордовые и белые/\\nНа фото цветы 2018г/\\nП. Зубчаниновка/\\nлибо пл. Революции/\\nЕсть ирисы, ромашка, клубника, боярышник и ирга'],\n",
       "       ['Кофта', 'Состояние отличное']], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-ZEdlEGAXTD"
   },
   "source": [
    "### Токенизация (1 балл)\n",
    "\n",
    "\n",
    "Токенизация -- разбиение текста на мелкие части, которые можно обработать машинными методами.\n",
    "Можно использовать разные алгоритмы токенизации.\n",
    "Давайте пока остановимся на простом WordPunctTokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "O9VgNlZ1Qy3o",
    "outputId": "bec90229-566f-4ccc-d7b1-605ee574cf87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: Здраствуйте. Я, Кирилл. Хотел бы чтобы вы сделали игру, 3Д-экшон суть такова...\n",
      "after: здраствуйте . я , кирилл . хотел бы чтобы вы сделали игру , 3д - экшон суть такова ...\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "\n",
    "def preprocess(text: str) -> str:\n",
    "    return ' '.join(tokenizer.tokenize(text.lower()))\n",
    "\n",
    "\n",
    "text = 'Здраствуйте. Я, Кирилл. Хотел бы чтобы вы сделали игру, 3Д-экшон суть такова...'\n",
    "print(\"before:\", text,)\n",
    "print(\"after:\", preprocess(text),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x_RYBKC26o1X"
   },
   "source": [
    "__Задание:__ Токенизируйте title и description в train и test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "Z5WO-7tJUvbs",
    "outputId": "bbfc731f-9859-4a67-c555-4b49a757d439"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['сапоги 46 размер новые', 'сапоги 46 размер новые'],\n",
       "       ['светильники потолочный swarovski',\n",
       "        'светильники потолочные swarovski 6 штук , цена за штуку . в эксплуатации 2 года , продаются в связи со сменой интерьера в квартире'],\n",
       "       ['iphone 7 plus 128gb red красный в наличии',\n",
       "        '/ / данная цена только для подписчиков instagram : iqmac / / новый красный айфон 7 plus в наличии это элегантный и мощный смартфон , который готов в полной мере раскрыть новые возможности ios 10 . аппарат с 4 - ядерным процессором а10 и 3 гб озу с легкостью решает самые ресурсоемкие задачи , позволяя наслаждаться быстродействием « тяжелых » приложений и игр на 5 , 5 - дюймовом дисплее . аппарат получил экран , как у ipad pro , так что картинка теперь соответствует кинематографическому стандарту .'],\n",
       "       ['пион ирис ромашка рассада',\n",
       "        'пион куст 500 р ( более 10 шт )/ саженец / корень 100р / растут у нас более 70 лет / розовые , бордовые и белые / на фото цветы 2018г / п . зубчаниновка / либо пл . революции / есть ирисы , ромашка , клубника , боярышник и ирга'],\n",
       "       ['кофта', 'состояние отличное'],\n",
       "       ['1 - к квартира , 33 м² , 4 / 5 эт .',\n",
       "        'продаётся уютная , тёплая квартира в экологически - чистом районе города , рядом сосновый бор , всегда чистый воздух . дом 2004 г ., хорошие соседи , на площадке 2 - е квартиры , развитая инфраструктура , в шаговой доступности поликлиника , школа , тк « орбита », вещевой рынок . квартира в хорошем состоянии . подходит под ипотеку , долгов , обременений , перепланировке нет . в квартире натяжные потолки , в ванной комнате стены выполнены из влагостойких стеновых панелей . возможен обмен на квартиру в г . магнитогорске , торг .'],\n",
       "       ['платье новое 60 размера',\n",
       "        'платье 60 размера , новое , красивого темно синего цвета , из трикотажной ткани : вискоза 95 %, эластина 5 % . а - образного силуэта с рукавом 2 / 3 . длинна по спинке 113см .'],\n",
       "       ['ваз 2114 samara , 2007',\n",
       "        'продам ваз 2114 2007 г . в . в хорошем состоянии . / 2 владельца , птс оригинал . / машина в родной краске , в дтп никогда не была ,/ днище целое не ржавое . по ходовой нареканий нет , сел и поехал . / имеется музыка , сигнализация 2 комплекта ключей , птф , передние стеклоподъемники ./ небольшой торг при осмотре . / обмен не интересует .'],\n",
       "       ['наушники блутус',\n",
       "        'долго держат заряд 4 - 5 часов , можно и больше при средней громкости выжать из них . вкладыши .'],\n",
       "       ['пальто tommy hilfiger',\n",
       "        'состояние нового . промахнулась с размером . пальто до - 10 - 12 градусов . / возможна пересылка по почте']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 95,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "for i in range(len(X_train)):\n",
    "    for j in range(len(X_train[i])):\n",
    "        X_train[i][j] = preprocess(X_train[i][j])\n",
    "for i in range(len(X_test)):\n",
    "    for j in range(len(X_test[i])):\n",
    "        X_test[i][j] = preprocess(X_test[i][j])\n",
    "X_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "kARGeJQwYTil",
    "outputId": "570c4985-6fe1-4363-b5e5-11ff7c4ac7fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['продам иж планета 3 , 76 год , ( стоит на старом учёте , документы утеряны ) на ходу , хорошее состояние , все интересующие вопросы по телефону ( с родной коляской на 3 тысячи дороже ) . торга не будет .'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[10:11, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VDnDSWwFDwFo"
   },
   "outputs": [],
   "source": [
    "assert X_train[10][1] == 'продам иж планета 3 , 76 год , ( стоит на старом учёте , документы утеряны ) на ходу , хорошее состояние , все интересующие вопросы по телефону ( с родной коляской на 3 тысячи дороже ) . торга не будет .'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hlIITUk0AsmS"
   },
   "source": [
    "### BOW (1.5 балла)\n",
    "\n",
    "Один из традиционных подходов -- построение bag of words.\n",
    "\n",
    "Метод состоит в следующем:\n",
    "\n",
    " - Составить словарь самых часто встречающихся слов в train data\n",
    " - Для каждого примера из train посчитать, сколько раз каждое слово из словаря в нём встречается\n",
    "\n",
    "\n",
    " В sklearn есть CountVectorizer, но в этом задании его использовать нельзя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GMKUttDWIF92"
   },
   "source": [
    "__Задание:__ Найдите k самых частых слов, отсортируйте их по убыванию частотности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RdcHRy9FElTg"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "list_of_words = []\n",
    "for i in X_train:\n",
    "    for j in i:\n",
    "        list_of_words += map(str, j.split())\n",
    "counts = Counter(list_of_words)\n",
    "bow_vocabulary2 = counts.most_common(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GxBTNGe_ElTr"
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in bow_vocabulary2:\n",
    "    result.append(i[0])\n",
    "sorted(result)[::200]\n",
    "assert sorted(result)[::200] == ['!', '12500', '270', '700', 'by', 'gh', 'michael', 'sonata', 'ø', 'аудиоподготовка', 'большим', 'веса', 'воспроизведения', 'габариты', 'гтд', 'джинсами', 'доступность', 'загрузки', 'зимней', 'использовался', 'квартала', 'коммуникации', 'кошки', 'лакированные', 'магазин', 'металл', 'мск', 'натуральным', 'носке', 'одному', 'отвечаем', 'пассат', 'плотно', 'покраску', 'постоянные', 'примеры', 'просьба', 'размещайте', 'репетитор', 'сантехник', 'сидения', 'современного', 'стала', 'схема', 'тон', 'удлиненная', 'фасад', 'цветами', 'шея', 'эту']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4awkhecbR9om"
   },
   "outputs": [],
   "source": [
    "def text_to_bow(text: str) -> np.array:\n",
    "    \"\"\"\n",
    "    Возвращает вектор, где для каждого слова из most_common\n",
    "    указано количество его употреблений\n",
    "    \"\"\" \n",
    "    list_of_words = text.split()\n",
    "    word_vector = []\n",
    "    count_words = Counter(list_of_words)\n",
    "    for i in result:\n",
    "        word_vector.append(count_words[i])\n",
    "    return np.array(word_vector)\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "thgzB4d5BEYs",
    "outputId": "84b2cd6f-9e7c-4a12-c365-28ce3653b62c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,    4,   12,  565,  866, 1601, 2539, 4063])"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.array(text_to_bow(\"сдаётся уютный , тёплый гараж для стартапов в ml\")) != 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IZnJT2JbdXA3"
   },
   "outputs": [],
   "source": [
    "assert np.allclose(np.where(text_to_bow(\"сдаётся уютный , тёплый гараж для стартапов в ml\") != 0)[0],\n",
    "                   np.array([   1,    4,   12,  565,  866, 1601, 2539, 4063]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HR_D8Fn4pudv"
   },
   "outputs": [],
   "source": [
    "def items_to_bow(items: np.array) -> np.array:\n",
    "    \"\"\" Для каждого товара возвращает вектор его bow \"\"\"\n",
    "    # Давайте для начала попробуем строить bow только из description товара\n",
    "    # assert ниже написан для bow из description\n",
    "    word_matrix = []\n",
    "    for i in items:\n",
    "        word_matrix.append(text_to_bow(i[1]))\n",
    "    return np.array(word_matrix)\n",
    "\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "W5GBh-HMj_Sh",
    "outputId": "9db75110-7d72-4ab3-e6ea-cd9507c93c65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    1,    2,    5,    6,    7,   12,   27,   41,   49,  110,\n",
       "         189,  208,  221, 2032, 3052, 7179, 9568]),)"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(items_to_bow([X_train[42]])[0] != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pKdfMqbIetPA"
   },
   "outputs": [],
   "source": [
    "assert np.allclose(np.where(items_to_bow([X_train[42]])[0] != 0),\n",
    "                   np.array([   0, 1, 2, 5, 6, 7, 12, 27, 41, 49, 110,\n",
    "                                189,  208,  221, 2032, 3052, 7179, 9568]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "colab_type": "code",
    "id": "wwOZaEpMSQsZ",
    "outputId": "e7aed33d-e478-4af8-8e65-a19a71510e0d"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d212dfaf1e10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train_bow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitems_to_bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_test_bow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitems_to_bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-4a93bc165b5c>\u001b[0m in \u001b[0;36mitems_to_bow\u001b[0;34m(items)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mword_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m       \u001b[0mword_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_to_bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-058fb89b5c09>\u001b[0m in \u001b[0;36mtext_to_bow\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mword_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcount_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m       \u001b[0mword_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train_bow = items_to_bow(X_train)\n",
    "X_test_bow = items_to_bow(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bJVLS8Fs3CeT"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vJoXiCWI7VF5"
   },
   "source": [
    "### Логистическая регрессия и SVC (1 балл)\n",
    "\n",
    "\n",
    "Теперь описание каждого товара представлено, как точка в многомерном пространстве.\n",
    "Очень важно запомнить эту идею: дальше мы будем рассматривать разные способы перехода от текста к точке в пространстве.\n",
    "\n",
    "Для BOW каждое измерение в пространстве -- какое-то слово.\n",
    "Мы предполагаем, что текст описывается набором каких-то популярных слов, которые в нём встречаются, а близкие по смыслу тексты будут использовать одинаковые слова.\n",
    "\n",
    "Обучите логистическую регрессию и SVC с базовыми параметрами.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "Ky3HV1rTSS9L",
    "outputId": "8f000ace-89ff-4e0c-8b2c-c19e0928580d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "bow_model = LogisticRegression(max_iter=100).fit(X_train_bow, y_train)\n",
    "accuracy_score(bow_model.predict(X_test_bow), y_test)\n",
    "\n",
    "prediction = bow_model.predict(X_test_bow)\n",
    "assert accuracy_score(prediction, y_test) > 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "grDLduQ6pEUA",
    "outputId": "8d199d65-e1ec-4e46-9ca4-4a213484d010"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7011111111111111"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(prediction, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "-c46ZT0lvF6T",
    "outputId": "4b1cb34a-201b-4dc2-9155-fdb6919c6c08"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6847777777777778"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "bow_model = LinearSVC(max_iter=70).fit(X_train_bow, y_train)\n",
    "print(accuracy_score(bow_model.predict(X_test_bow), y_test))\n",
    "\n",
    "assert accuracy_score(bow_model.predict(X_test_bow), y_test) > 0.68"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WwKE57YZ1Hzn"
   },
   "source": [
    "### Модификация признаков (0.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ewMlxQezL6Ax"
   },
   "source": [
    "Добавьте title товара в bow с произвольным весом, как изменится качество?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "colab_type": "code",
    "id": "evqKo1r5L5BO",
    "outputId": "b172424c-3b39-4650-d35d-8cfd8821b3f8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-17e4fa12f66b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mall_items_to_bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\" Для каждого товара возвращает вектор его bow \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Давайте для начала попробуем строить bow только из description товара\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# assert ниже написан для bow из description\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mword_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "def all_items_to_bow(items: np.array) -> np.array:\n",
    "    \"\"\" Для каждого товара возвращает вектор его bow \"\"\"\n",
    "    # добавляем title к bow с весом 1\n",
    "    word_matrix = []\n",
    "    for i in items:\n",
    "        word_matrix.append(text_to_bow(i[0] + ' ' + i[1]))\n",
    "    return np.array(word_matrix)\n",
    "\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N9H94UBHrDbs"
   },
   "outputs": [],
   "source": [
    "X_train_title_bow = all_items_to_bow(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bNQsjMwwtRUm"
   },
   "outputs": [],
   "source": [
    "X_test_title_bow = all_items_to_bow(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "WpjXPon_rQ2i",
    "outputId": "034aeb70-77fe-444d-b482-3b19d9f4f585"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7812222222222223"
      ]
     },
     "execution_count": 68,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "bow_model = LogisticRegression(max_iter=100).fit(X_train_title_bow, y_train)\n",
    "accuracy_score(bow_model.predict(X_test_title_bow), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "8oPQcokJsKFV",
    "outputId": "c8e22cff-9dba-42e2-d41a-35462ca0da22"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7553333333333333"
      ]
     },
     "execution_count": 69,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "bow_model = LinearSVC(max_iter=70).fit(X_train_title_bow, y_train)\n",
    "accuracy_score(bow_model.predict(X_test_title_bow), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Db4TyqzxMnby"
   },
   "source": [
    "Нормализуйте данные (`sklearn.preprocessing.normalize`) перед обучением. Что станет с качеством и почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A8rVy6q1Mn4J"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from sklearn.preprocessing import normalize\n",
    "X_train_norm = normalize(X_train_title_bow)\n",
    "X_test_norm = normalize(X_test_title_bow)\n",
    "del X_train_title_bow\n",
    "del X_test_title_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "7JPpXzw9vuis",
    "outputId": "731c195a-a970-45b0-aa8f-da9eeae51799"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.677"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "bow_model = LogisticRegression(max_iter=100).fit(X_train_norm, y_train)\n",
    "accuracy_score(bow_model.predict(X_test_norm), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zK5oGBklv0hI",
    "outputId": "29a2760a-89dc-4f79-bdf6-b2824668a448"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7987777777777778"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "bow_model = LinearSVC(max_iter=70).fit(X_train_norm, y_train)\n",
    "accuracy_score(bow_model.predict(X_test_norm), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# качество на логистической регрессии понизилось после нормализации, на методе опорных векторов повысилось."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HvCAL3qGDByj"
   },
   "source": [
    "### mystem (0.5) балла\n",
    "\n",
    "Попробуйте обучиться, используя токенизатор mystem. Сравните качество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hz38TqqRDY6-"
   },
   "outputs": [],
   "source": [
    "!wget http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
    "!tar -xvf mystem-3.0-linux3.1-64bit.tar.gz\n",
    "!cp mystem /bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60oQ-6UgDcLF"
   },
   "outputs": [],
   "source": [
    "pip install git+https://github.com/nlpub/pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mGvNHfVsDfhq"
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bXbsPtpfoB7m"
   },
   "source": [
    "### TF-IDF (1.5 балла)\n",
    "\n",
    "Не все слова полезны одинаково, давайте попробуем [взвесить](http://tfidf.com/) их, чтобы отобрать более полезные.\n",
    "\n",
    "\n",
    "> TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "> \n",
    "> IDF(t) = log_e(Total number of documents / Number of documents with term t in it).\n",
    "\n",
    "\n",
    "В sklearn есть TfidfVectorizer, но в этом задании его использовать нельзя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "colab_type": "code",
    "id": "_yIeoic7o3ES",
    "outputId": "2a5fe613-6881-440b-9b72-9ee8ac5adf10",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10383., 14742., 16346.,  9671., 19347., 20154., 15227.,  5697.,\n",
       "        4903., 19681.,  5609., 13071.,  5535., 10044.,  5859., 10399.,\n",
       "       11073.,  4385.,  9293., 12603.,   675.,  1826.,  7931.,  8600.,\n",
       "        3818.,  2122., 17098.,  9398.,  3928.,  3767.,  3234.,  5638.,\n",
       "         757.,  7620.,  2352.,  2080.,  6385.,  4297., 19587.,  2434.,\n",
       "        2584.,  3348.,  1698.,  4502.,  2266.,  1507., 15855.,   211.,\n",
       "        1815.,  2046.,  6555.,  4764.,  1798., 20123.,  1822.,  1641.,\n",
       "        1801.,  2126., 19018.,  1140.,   368.,  1231.,  1466.,  6054.,\n",
       "        1547.,  4889.,  5253.,  1556.,  1454.,  1306.,  1460.,  5747.,\n",
       "        5062.,   859., 20492.,   959.,  2630.,   543.,  2885., 14423.,\n",
       "         661.,   715.,  1433.,  1045.,  1980.,  1613.,   667.,  1124.,\n",
       "        9144.,  1700.,  1907.,  2075.,  1111.,  1086.,  3377.,    75.,\n",
       "        1792.,   990.,   938.,  1070.,  1863.,  1193.,   901.,  1006.,\n",
       "        2041., 11012.,  2127.,  1093.,  1157.,  1521.,   848.,   865.,\n",
       "         951.,  3908.,  2293.,   862.,   833.,  2314.,   750.,   598.,\n",
       "        2362.,   545., 18268.,  6272.,   866.,   785.,   237.,   900.,\n",
       "         899.,   784.,  1389.,  6979.,   831., 11850.,  1336.,  1599.,\n",
       "         727.,   932.,   906.,   602.,  1869.,   728.,   739.,  1879.,\n",
       "        2541., 15678., 11539.,   465.,   482.,   760.,   845.,  1883.,\n",
       "         152., 20115.,  2487.,   687.,   534.,  1224.,   726.,  2436.,\n",
       "         779.,   647.,   671.,   883.,  1764.,   701.,   886.,   486.,\n",
       "         657.,   433.,   625.,   665.,  2095.,   699.,   561.,   815.,\n",
       "         795.,   593.,  1607.,  1876.,   493.,   539.,  2421.,   573.,\n",
       "         618.,   549.,  1354.,  1534.,   555.,   773.,   515.,   598.,\n",
       "        1450.,   577.,   682.,   409.,   474.,   627., 14623.,   476.])"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Давайте для простоты считать один tf-idf для title и description.\n",
    "# Для каждого слова из bow_vocabulary нужно посчитать\n",
    "# в тексте скольких товаров встретилось это слово\n",
    "\n",
    "\n",
    "word_vector = np.zeros(len(result))\n",
    "for i in range(len(result)):\n",
    "    for j in X_train:\n",
    "        if result[i] in j[0] or result[i] in j[1]:\n",
    "            word_vector[i] += 1\n",
    "\n",
    "# count_arr = #\n",
    "word_vector[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6i5zFpD9rbtz"
   },
   "outputs": [],
   "source": [
    "def text_to_tfidf(text: str) -> np.array:\n",
    "    \"\"\"\n",
    "    Возвращает вектор, где для каждого слова из most_common\n",
    "    указан tf-idf\n",
    "    \"\"\"\n",
    "    list_of_words = text.split()\n",
    "    dct_words = Counter(list_of_words)\n",
    "    tf_idf_vector = []\n",
    "    for i in range(len(result)):\n",
    "        tf = dct_words[result[i]] / len(list_of_words)\n",
    "        idf = np.log(len(X_train) / word_vector[i])\n",
    "        tf_idf_vector.append(tf*idf)\n",
    "    return tf_idf_vector\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LvL9BH7DsJrv"
   },
   "outputs": [],
   "source": [
    "tf_idf_features = []\n",
    "for i in X_train:\n",
    "    tf_idf_features += text_to_tfidf(i[0] + ' ' + i[1])\n",
    "tf_idf_ft = np.array(tf_idf_features).reshape((len(tf_idf_features) // len(result), len(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mrSxmsZbMRsM"
   },
   "outputs": [],
   "source": [
    "# Нормализуйте данные\n",
    "from sklearn.preprocessing import normalize\n",
    "X_train_tfidf_norm = normalize(tf_idf_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ul1W8eKfOvxM"
   },
   "outputs": [],
   "source": [
    "del tf_idf_features\n",
    "del tf_idf_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_871REoQO6V4"
   },
   "outputs": [],
   "source": [
    "tf_idf_features = []\n",
    "for i in X_test:\n",
    "    tf_idf_features += text_to_tfidf(i[0] + ' ' + i[1])\n",
    "tf_idf_ft = np.array(tf_idf_features).reshape((len(tf_idf_features) // len(result), len(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5127tUHDElWP"
   },
   "outputs": [],
   "source": [
    "# Нормализуйте данные\n",
    "X_test_tfidf_norm = normalize(tf_idf_ft)\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UXPPIcYUKtRK"
   },
   "outputs": [],
   "source": [
    "del tf_idf_features\n",
    "del tf_idf_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JQZHbuhJQJHt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-YFA-8kE1RHk"
   },
   "source": [
    "### Модели на TF-IDF признаках (1 балл)\n",
    "\n",
    "Обучите логистическую регрессию и SVC, оцените качество (accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "-ULrXsF1m5sU",
    "outputId": "9bb72e02-4a09-4d7d-f2fc-3a7c2937bd6f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7643333333333333"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "tfidf_model = LogisticRegression(max_iter=100).fit(X_train_tfidf_norm, y_train)\n",
    "accuracy_score(tfidf_model.predict(X_test_tfidf_norm), y_test)\n",
    "\n",
    "# /usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
    "# STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
    "\n",
    "# Increase the number of iterations (max_iter) or scale the data as shown in:\n",
    "#     https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "# Please also refer to the documentation for alternative solver options:\n",
    "#     https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "#   extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
    "\n",
    "# 0.7643333333333333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aJOqHklAQqGF",
    "outputId": "dc2cd159-2039-4524-ef77-4710c7233725"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8057777777777778"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "tfidf_model = LinearSVC(max_iter=70).fit(X_train_tfidf_norm, y_train)\n",
    "accuracy_score(tfidf_model.predict(X_test_tfidf_norm), y_test)\n",
    "\n",
    "# 0.8057777777777778"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iC5uX6yRSCw1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jFdy3lUFDsOr"
   },
   "source": [
    "### Hashing Vectorizer (0.5 балла)\n",
    "\n",
    "Попробуйте использовать `sklearn.feature_extraction.text.HashingVectorizer` для векторизации текстов.\n",
    "Обязательно оцените качество работы алгоритмов классификации с использованием новой векторизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y666HTrqDq1m"
   },
   "outputs": [],
   "source": [
    "def x_for_hv(items: np.array) -> np.array:\n",
    "    list_for_hv = []\n",
    "    for i in items:\n",
    "        list_for_hv.append(i[0] + ' ' + i[1])\n",
    "    return list_for_hv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nKvHJtHmXCQY"
   },
   "outputs": [],
   "source": [
    "X_train_for_hv = x_for_hv(X_train)\n",
    "X_test_for_hv = x_for_hv(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "v9dB8AjsWEmw",
    "outputId": "c6e1e7ae-7fe3-4238-9f81-fb0f25cf2ae6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21000, 16384)\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "vectorizer = HashingVectorizer(n_features=2**14)\n",
    "X_train_hv = vectorizer.fit_transform(X_train_for_hv)\n",
    "print(X_train_hv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6A-IVDMKXyQ3",
    "outputId": "3c3518fb-52a7-4561-b953-55db46205bcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 16384)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = HashingVectorizer(n_features=2**14)\n",
    "X_test_hv = vectorizer.fit_transform(X_test_for_hv)\n",
    "print(X_test_hv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "S9BQ0ixoX9f7",
    "outputId": "1a02e31a-944f-45ec-da8e-1eb3810ec8a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7277777777777777"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hv_model = LogisticRegression(max_iter=100).fit(X_train_hv, y_train)\n",
    "accuracy_score(hv_model.predict(X_test_hv), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-I6b69OTYuMs",
    "outputId": "b6469854-cf3e-40ef-df32-9d7e309689b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8076666666666666"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hv_model = LinearSVC(max_iter=70).fit(X_train_hv, y_train)\n",
    "accuracy_score(hv_model.predict(X_test_hv), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A0SclNcYV22b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vQZ61xSsTpZI"
   },
   "source": [
    "### Word Vectors (1 балл)\n",
    "\n",
    "Давайте попробуем другой подход -- кажому слову сопоставим какой-то эмбеддинг (вектор).\n",
    "\n",
    "Вектора будут небольшой размерности. Таким образом мы снизим количество параметров в модели.\n",
    "\n",
    "Вектора мы возьмём уже готовые (обученные на текстах их интернета), так что наша модель будет знать некоторую дополнительную информацию о внешнем мире."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "id": "T38J27NcYGx5",
    "outputId": "7af98fc6-1afd-4ba0-f0a9-44ae49a1391e",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-04-15 15:22:33--  https://www.dropbox.com/s/0x7oxso6x93efzj/ru.tar.gz\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.82.1, 2620:100:6032:1::a27d:5201\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.82.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/raw/0x7oxso6x93efzj/ru.tar.gz [following]\n",
      "--2020-04-15 15:22:34--  https://www.dropbox.com/s/raw/0x7oxso6x93efzj/ru.tar.gz\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc1f9f3824b5d9dd950ccb4f16fa.dl.dropboxusercontent.com/cd/0/inline/A14dmMZHW5K0ZKZXQ1Hl0ul20vpSkqrbmrC0P14VBznFVYTS---2BUuCgateNJH56VSEmvuwgiuLyVg9KM7rerUWPMTcsBXYI8u6L9Qqsjv4mA/file# [following]\n",
      "--2020-04-15 15:22:34--  https://uc1f9f3824b5d9dd950ccb4f16fa.dl.dropboxusercontent.com/cd/0/inline/A14dmMZHW5K0ZKZXQ1Hl0ul20vpSkqrbmrC0P14VBznFVYTS---2BUuCgateNJH56VSEmvuwgiuLyVg9KM7rerUWPMTcsBXYI8u6L9Qqsjv4mA/file\n",
      "Resolving uc1f9f3824b5d9dd950ccb4f16fa.dl.dropboxusercontent.com (uc1f9f3824b5d9dd950ccb4f16fa.dl.dropboxusercontent.com)... 162.125.82.6, 2620:100:6032:6::a27d:5206\n",
      "Connecting to uc1f9f3824b5d9dd950ccb4f16fa.dl.dropboxusercontent.com (uc1f9f3824b5d9dd950ccb4f16fa.dl.dropboxusercontent.com)|162.125.82.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 FOUND\n",
      "Location: /cd/0/inline2/A14rx6pxxGZj2FrwTgd386XPqPeg8-lYPEP2Hyer6VPQcURSD3UZ_qO1p28W6vN8M7VAUx16YeM4jYzcpV_dsUJs1YC3xcAyFrcW7XOjZEvXfh5oLQtxRA6ACxzleYuaTMwpQ4emXeTEZpRWW_8XpfWcqp7PmmMPyddJaTAVwcW0_kkvbnKpCGJ2m4C6BnI0IAGM2AZvvxeY5an1IBSWYPyndCkpZGGxpS08BUhVzTzVTViRmz9QIi069fKAE7Svpbf8AlkjkVXFaPLg6qZpMkBlh5CjmCfsye8mDUHxNasfgPBvrnZjwakrnSZ0cgkR2jP7ipsSwen2zlbJgetc4ufU/file [following]\n",
      "--2020-04-15 15:22:35--  https://uc1f9f3824b5d9dd950ccb4f16fa.dl.dropboxusercontent.com/cd/0/inline2/A14rx6pxxGZj2FrwTgd386XPqPeg8-lYPEP2Hyer6VPQcURSD3UZ_qO1p28W6vN8M7VAUx16YeM4jYzcpV_dsUJs1YC3xcAyFrcW7XOjZEvXfh5oLQtxRA6ACxzleYuaTMwpQ4emXeTEZpRWW_8XpfWcqp7PmmMPyddJaTAVwcW0_kkvbnKpCGJ2m4C6BnI0IAGM2AZvvxeY5an1IBSWYPyndCkpZGGxpS08BUhVzTzVTViRmz9QIi069fKAE7Svpbf8AlkjkVXFaPLg6qZpMkBlh5CjmCfsye8mDUHxNasfgPBvrnZjwakrnSZ0cgkR2jP7ipsSwen2zlbJgetc4ufU/file\n",
      "Reusing existing connection to uc1f9f3824b5d9dd950ccb4f16fa.dl.dropboxusercontent.com:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2399456034 (2.2G) [application/octet-stream]\n",
      "Saving to: ‘ru.tar.gz’\n",
      "\n",
      "ru.tar.gz           100%[===================>]   2.23G  48.4MB/s    in 66s     \n",
      "\n",
      "2020-04-15 15:23:41 (34.5 MB/s) - ‘ru.tar.gz’ saved [2399456034/2399456034]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/0x7oxso6x93efzj/ru.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Zfse4xVbgMIr",
    "outputId": "3263a3c1-00ff-4dd3-de87-631df0bd8511"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ru.bin\n",
      "ru.vec\n"
     ]
    }
   ],
   "source": [
    "!tar -xzvf ru.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sy2TXmQ2jZSY"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.wrappers import FastText\n",
    "\n",
    "\n",
    "model = FastText.load_fasttext_format('ru.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H49QR_jhjmCa"
   },
   "outputs": [],
   "source": [
    "# Эмбеддинг предложения -- сумма эмбеддингов токенов\n",
    "\n",
    "\n",
    "def sentence_embedding(sentence: str) -> np.array:\n",
    "    \"\"\"\n",
    "    Складывает вектора токенов строки sentence\n",
    "    \"\"\"\n",
    "\n",
    "    embedding_dim = model['кек'].shape[0]\n",
    "    features = np.zeros([embedding_dim], dtype='float32')\n",
    "    \n",
    "    for word in sentence.split():\n",
    "        if word in model:\n",
    "            features += model[word]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gj6U_hjtlllV"
   },
   "outputs": [],
   "source": [
    "assert np.allclose(sentence_embedding('сдаётся уютный , тёплый гараж для стартапов в ml')[::50],\n",
    "                   np.array([ 0.08189847,  0.07249198, -0.15601222,  0.03782297,  0.09215296, -0.23092946]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KJkYxxfwcgoK"
   },
   "outputs": [],
   "source": [
    "train_items_embedding = []\n",
    "for i in X_train:\n",
    "    train_items_embedding.append(sentence_embedding(i[0] + ' ' + i[1]))\n",
    "train_items_emb = np.array(train_items_embedding)\n",
    "\n",
    "test_items_embedding = []\n",
    "for i in X_test:\n",
    "    test_items_embedding.append(sentence_embedding(i[0] + ' ' + i[1]))\n",
    "test_items_emb = np.array(test_items_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "9H2zcmdBpjqd",
    "outputId": "0ffde90c-797a-468b-ab86-1a0a7f7bde33"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5548888888888889"
      ]
     },
     "execution_count": 63,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_model = LogisticRegression(max_iter=100).fit(train_items_emb, y_train)\n",
    "accuracy_score(emb_model.predict(test_items_emb), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "RVpRvq5tc6m4",
    "outputId": "c0c602f8-1f10-48e4-f8f7-9419dc07ec83"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5333333333333333"
      ]
     },
     "execution_count": 64,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_model = LinearSVC(max_iter=70).fit(train_items_emb, y_train)\n",
    "accuracy_score(emb_model.predict(test_items_emb), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8tfhc-PFmGvu"
   },
   "outputs": [],
   "source": [
    "# Обучите логистическую регрессию и SVM\n",
    "# Оцените качество (accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cVEdlFostSnX"
   },
   "source": [
    "### Что дальше?\n",
    "\n",
    "Решение каждого пункта 1 балл:\n",
    "\n",
    "1. N-Gram модели текстовой классификации\n",
    "\n",
    "2. Обучиться на полных данных (контест на kaggle)\n",
    "\n",
    "3. Поработать с другими эмбеддингами (word2vec, GloVe).\n",
    "\n",
    "4. Использовать Vowpal Wabbit вместо sklearn.\n",
    "\n",
    "5. Другие способы токенизации (pymorphy2, spaCy)\n",
    "\n",
    "\n",
    "Снабжайте код пояснениями и графиками.\n",
    "Обязательно необходимо написать вывод по каждому пункту, который вы реализуете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "xyIh2PN3udhD",
    "outputId": "bfea52e2-5735-423a-d476-64438a7f6adb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "train_for_w2v = []\n",
    "for i in X_train:\n",
    "    train_for_w2v.append(i[0] + i[1]) # создаем тренировочную выборку для модели word2vec из title и description\n",
    "\n",
    "w2v = word2vec.Word2Vec(train_for_w2v, size=300, window=3, workers=4) # обучаем модель на тренировочной выборке, длина векторов = 300\n",
    "model = dict(zip(w2v.wv.index2word, w2v.wv.syn0)) # создаем словарь, где слову сопоставлен вектор, из полученной выше модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CzgQs0HH-6Hd"
   },
   "outputs": [],
   "source": [
    "del train_for_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c7RCn3Te_Gr1"
   },
   "outputs": [],
   "source": [
    "def w2v_embedding(sentence: str) -> np.array:\n",
    "    \"\"\"\n",
    "    Складывает вектора токенов строки sentence\n",
    "    \"\"\"\n",
    "\n",
    "    features = np.zeros([300], dtype='float32')\n",
    "    \n",
    "    for word in sentence.split():\n",
    "        if word in model:\n",
    "            features += model[word]\n",
    "    \n",
    "    return features # аналогично FastText, получаем эмбеддинг предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-skQav2W_3eG"
   },
   "outputs": [],
   "source": [
    "# преобразуем каждое предложение в вектор\n",
    "X_train_w2v = []\n",
    "for i in X_train:\n",
    "    X_train_w2v.append(w2v_embedding(i[0] + ' ' + i[1]))\n",
    "X_train_word2vec = np.array(X_train_w2v)\n",
    "\n",
    "X_test_w2v = []\n",
    "for i in X_test:\n",
    "    X_test_w2v.append(w2v_embedding(i[0] + ' ' + i[1]))\n",
    "X_test_word2vec = np.array(X_test_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6A4zpgKzAbtm",
    "outputId": "d924d76f-39c3-4dfb-d231-549fb057b6f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21000, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_word2vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "QVo9PJ9hAiHH",
    "outputId": "2552c21a-02b5-4b0b-fd6e-0e72d998e8f4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3042222222222222"
      ]
     },
     "execution_count": 99,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# обучаем логистическую регрессию, смотрим результаты\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "w2v_model = LogisticRegression(max_iter=100).fit(X_train_word2vec, y_train)\n",
    "accuracy_score(w2v_model.predict(X_test_word2vec), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "M1_9R0pZA2G5",
    "outputId": "973df482-0aa1-4490-f994-be3e14fbd7a1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.14466666666666667"
      ]
     },
     "execution_count": 100,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# обучаем LinearSVC, смотрим результаты\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "w2v_model = LinearSVC(max_iter=70).fit(X_train_word2vec, y_train)\n",
    "accuracy_score(w2v_model.predict(X_test_word2vec), y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cOZiDYqzRyZ4"
   },
   "outputs": [],
   "source": [
    "# наблюдается заметное снижение качества. Возможно, увеличение количества итераций при обучении, удаление стоп-слов улучшило бы качество. Сейчас алгоритм word2vec не находит взаимосвязи между словами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2x5Z4d5kBrU6"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "# составим словарь триграмм, ключи словаря - пары последовательных слов в предложении, значения - словари, \n",
    "# в которых ключи это слова, идущие после биграмм (третьи слова в триграмме), значения - количество триграмм, образованных этими тремя словами. \n",
    "for sentence in np.hstack((X_train[:, 0], X_train[:, 1])):\n",
    "    sentence_split = [i for i in sentence.split()]\n",
    "    for i in range(len(sentence_split) - 2):\n",
    "        model[(sentence_split[i], sentence_split[i + 1])][sentence_split[i + 2]] += 1\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mYBYdT6vSmyW"
   },
   "outputs": [],
   "source": [
    "# заменим значения в словарях с количества триграмм в тексте на долю триграмм от всех триграмм, имеющих аналогичное начало\n",
    "for w1_w2 in model:\n",
    "    total_count = float(sum(model[w1_w2].values()))\n",
    "    for w3 in model[w1_w2]:\n",
    "        model[w1_w2][w3] /= total_count\n",
    "# получили NGram модель    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Sfc9A0YqgdQr",
    "outputId": "329a237e-eebc-4df3-adb9-8d3b0c3bf255"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>.<locals>.<lambda>>, {'размер': 1.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['сапоги', '46']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QZ_pu6ufbyyJ"
   },
   "outputs": [],
   "source": [
    "# получаем probability для title и description каждого объекта тестовой выборки\n",
    "result_probabilities = []\n",
    "for i in X_test:\n",
    "    title_prob = 1\n",
    "    descr_prob = 1\n",
    "    for j in range(len(i)):\n",
    "        sentence_split = [k for k in i[j].split()]\n",
    "        for k in range(len(sentence_split) - 2):\n",
    "            if (sentence_split[k], sentence_split[k + 1]) in model:\n",
    "                if sentence_split[k + 2] in model[sentence_split[k], sentence_split[k + 1]]:\n",
    "                    if j == 0:\n",
    "                        title_prob *= model[sentence_split[k], sentence_split[k + 1]][sentence_split[k + 2]]\n",
    "                    else:\n",
    "                        descr_prob *= model[sentence_split[k], sentence_split[k + 1]][sentence_split[k + 2]]\n",
    "    result_probabilities.append([title_prob, descr_prob])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7zuchpo6kB6y",
    "outputId": "785020fa-c90d-46e9-d47e-44541df81a90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.038461538461538464, 6.496309225032204e-28]"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_probabilities[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "colab_type": "code",
    "id": "99C0_sS8e-18",
    "outputId": "997c48c6-2821-4dee-e026-dd364328cba3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymorphy2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
      "\r",
      "\u001b[K     |███████                         | 10kB 20.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▏                 | 20kB 1.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 30kB 2.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▍   | 40kB 2.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 51kB 1.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.6.2)\n",
      "Collecting pymorphy2-dicts<3.0,>=2.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1MB 6.9MB/s \n",
      "\u001b[?25hCollecting dawg-python>=0.7\n",
      "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
      "Installing collected packages: pymorphy2-dicts, dawg-python, pymorphy2\n",
      "Successfully installed dawg-python-0.7.2 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985\n"
     ]
    }
   ],
   "source": [
    "# другие способы токенизации - pymorphy2 и spaCy\n",
    "pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kzBrObGhbpDG"
   },
   "outputs": [],
   "source": [
    "from pymorphy2.tokenizers import simple_word_tokenize\n",
    "\n",
    "# функция для токенизации предложения с помощью pymorphy2\n",
    "def preprocess_pm(text: str) -> str:\n",
    "    return ' '.join(simple_word_tokenize(text.lower()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o90k-IkxnVeG"
   },
   "outputs": [],
   "source": [
    "new_data = pd.read_csv(\"train_subset.csv\", index_col='id')\n",
    "X = new_data[['title', 'description']].to_numpy()\n",
    "y = new_data['Category'].to_numpy()\n",
    "\n",
    "del new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z86aC9vxn0lF"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "vfdxXxainwXj",
    "outputId": "0e558866-cd1e-422a-a0b8-7e6a76740d64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['сапоги 46 размер новые', 'сапоги 46 размер новые'],\n",
       "       ['светильники потолочный swarovski',\n",
       "        'светильники потолочные swarovski 6 штук , цена за штуку . в эксплуатации 2 года , продаются в связи со сменой интерьера в квартире'],\n",
       "       ['iphone 7 plus 128gb red красный в наличии',\n",
       "        '/ / данная цена только для подписчиков instagram : iqmac / / новый красный айфон 7 plus в наличии это элегантный и мощный смартфон , который готов в полной мере раскрыть новые возможности ios 10 . аппарат с 4-ядерным процессором а10 и 3 гб озу с легкостью решает самые ресурсоемкие задачи , позволяя наслаждаться быстродействием « тяжелых » приложений и игр на 5 , 5-дюймовом дисплее . аппарат получил экран , как у ipad pro , так что картинка теперь соответствует кинематографическому стандарту .'],\n",
       "       ['пион ирис ромашка рассада',\n",
       "        'пион куст 500 р ( более 10 шт ) / саженец / корень 100р / растут у нас более 70 лет / розовые , бордовые и белые / на фото цветы 2018г / п . зубчаниновка / либо пл . революции / есть ирисы , ромашка , клубника , боярышник и ирга'],\n",
       "       ['кофта', 'состояние отличное'],\n",
       "       ['1-к квартира , 33 м² , 4 / 5 эт .',\n",
       "        'продаётся уютная , тёплая квартира в экологически-чистом районе города , рядом сосновый бор , всегда чистый воздух . дом 2004 г . , хорошие соседи , на площадке 2-е квартиры , развитая инфраструктура , в шаговой доступности поликлиника , школа , тк « орбита » , вещевой рынок . квартира в хорошем состоянии . подходит под ипотеку , долгов , обременений , перепланировке нет . в квартире натяжные потолки , в ванной комнате стены выполнены из влагостойких стеновых панелей . возможен обмен на квартиру в г . магнитогорске , торг .'],\n",
       "       ['платье новое 60 размера',\n",
       "        'платье 60 размера , новое , красивого темно синего цвета , из трикотажной ткани : вискоза 95 % , эластина 5 % . а-образного силуэта с рукавом 2 / 3 . длинна по спинке 113см .'],\n",
       "       ['ваз 2114 samara , 2007',\n",
       "        'продам ваз 2114 2007 г . в . в хорошем состоянии . / 2 владельца , птс оригинал . / машина в родной краске , в дтп никогда не была , / днище целое не ржавое . по ходовой нареканий нет , сел и поехал . / имеется музыка , сигнализация 2 комплекта ключей , птф , передние стеклоподъемники . / небольшой торг при осмотре . / обмен не интересует .'],\n",
       "       ['наушники блутус',\n",
       "        'долго держат заряд 4-5 часов , можно и больше при средней громкости выжать из них . вкладыши .'],\n",
       "       ['пальто tommy hilfiger',\n",
       "        'состояние нового . промахнулась с размером . пальто до -10 -12 градусов . / возможна пересылка по почте']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(X_train)):\n",
    "    for j in range(len(X_train[i])):\n",
    "        X_train[i][j] = preprocess_pm(X_train[i][j])\n",
    "for i in range(len(X_test)):\n",
    "    for j in range(len(X_test[i])):\n",
    "        X_test[i][j] = preprocess_pm(X_test[i][j])\n",
    "X_train[:10]\n",
    "# токенизовали все предложения в тренировочной и тестовой выборках"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q5TMcXShoD8b"
   },
   "outputs": [],
   "source": [
    "list_of_words = []\n",
    "for i in X_train:\n",
    "    for j in i:\n",
    "        list_of_words += map(str, j.split())\n",
    "counts = Counter(list_of_words)\n",
    "bow_vocabulary2 = counts.most_common(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E3EdQQc8pRmQ"
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in bow_vocabulary2:\n",
    "    result.append(i[0])\n",
    "# составли список самых частовстречающихся слов в выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KWNf0e17pxcn"
   },
   "outputs": [],
   "source": [
    "X_train_bow_pm = items_to_bow(X_train)\n",
    "X_test_bow_pm = items_to_bow(X_test)\n",
    "# каждый объект выборок представляем в виде вектора способом bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "AHddN1Ftp5Zr",
    "outputId": "750799b3-2d04-4bc7-8728-71261f81dab0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6472222222222223"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "pm_bow_model = LogisticRegression(max_iter=100).fit(X_train_bow_pm, y_train)\n",
    "accuracy_score(pm_bow_model.predict(X_test_bow_pm), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "X9LJlHoUqFDn",
    "outputId": "ffa7bca1-ef20-49ec-c296-f5bd88b444f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6824444444444444"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "pm_bow_model = LinearSVC(max_iter=70).fit(X_train_bow_pm, y_train)\n",
    "accuracy_score(pm_bow_model.predict(X_test_bow_pm), y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Oaz9TkHS_Hz"
   },
   "outputs": [],
   "source": [
    "# после обучения оценили качество. Оно ниже чем при токенизации в первом задании в случае с\n",
    "# логистической регрессией и близко к качеству на методе опорных векторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d6LVgRi_t1w6"
   },
   "outputs": [],
   "source": [
    "del X_test_bow_pm\n",
    "del X_train_bow_pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 558
    },
    "colab_type": "code",
    "id": "wBw3gslntsF8",
    "outputId": "61e84320-0c6b-4819-a7c8-5af22829a408"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/aatimofeev/spacy_russian_tokenizer.git\n",
      "  Cloning https://github.com/aatimofeev/spacy_russian_tokenizer.git to /tmp/pip-req-build-mucpi5vb\n",
      "  Running command git clone -q https://github.com/aatimofeev/spacy_russian_tokenizer.git /tmp/pip-req-build-mucpi5vb\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from spacy-russian-tokenizer==0.1.1) (2.2.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (4.38.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (3.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (2.0.3)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (7.4.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (1.18.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (1.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (2.21.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (46.1.3)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (1.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (0.6.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->spacy-russian-tokenizer==0.1.1) (1.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->spacy-russian-tokenizer==0.1.1) (2020.4.5.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->spacy-russian-tokenizer==0.1.1) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->spacy-russian-tokenizer==0.1.1) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->spacy-russian-tokenizer==0.1.1) (2.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->spacy-russian-tokenizer==0.1.1) (3.1.0)\n",
      "Building wheels for collected packages: spacy-russian-tokenizer\n",
      "  Building wheel for spacy-russian-tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for spacy-russian-tokenizer: filename=spacy_russian_tokenizer-0.1.1-cp36-none-any.whl size=12675 sha256=34271d77cd72fb85e284ad5b127addd76df05e01452c6bb19e8bf6d8856fb91e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-u7iy5f1q/wheels/37/3b/bb/cfe712f7c0b78cd08f4a2ef122d17748baf9d4bebecf2e5a54\n",
      "Successfully built spacy-russian-tokenizer\n",
      "Installing collected packages: spacy-russian-tokenizer\n",
      "Successfully installed spacy-russian-tokenizer-0.1.1\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/aatimofeev/spacy_russian_tokenizer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "atWEN9tvuG7J"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rE5NVHKjtpwM"
   },
   "outputs": [],
   "source": [
    "from spacy.lang.ru import Russian\n",
    "# используем библиотеку для токенизации русскоязычного текста\n",
    "\n",
    "def preprocess_spacy(text: str) -> str:\n",
    "    nlp = Russian()\n",
    "    return ' '.join([token.text for token in nlp(text)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9XPex3Nlz_tE",
    "outputId": "3885c889-1161-48ac-ae57-4cfbf66df724"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'сегодня , хорошая ! погода'"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_spacy('сегодня, хорошая! погода')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4IzCOGrAu-_D"
   },
   "outputs": [],
   "source": [
    "for i in range(len(X_train)):\n",
    "    for j in range(len(X_train[i])):\n",
    "        X_train[i][j] = preprocess_spacy(X_train[i][j])\n",
    "# токенизовали все предложения тренировочной выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "0ZoC8R2T92CZ",
    "outputId": "6c672f3d-2fac-4715-daf6-614091319d03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['сапоги 46 размер новые', 'сапоги 46 размер новые'],\n",
       "       ['светильники потолочный swarovski',\n",
       "        'светильники потолочные swarovski 6 штук , цена за штуку . в эксплуатации 2 года , продаются в связи со сменой интерьера в квартире'],\n",
       "       ['iphone 7 plus 128 gb red красный в наличии',\n",
       "        '\\xa0      / \\n      / \\n       данная цена только для подписчиков instagram : iqmac/ \\n      / \\n       новый красный айфон 7 plus в наличии это элегантный и мощный смартфон , который готов в полной мере раскрыть новые возможности ios 10 . аппарат с 4-ядерным процессором а10 и 3 гб озу с легкостью решает самые ресурсоемкие задачи , позволяя наслаждаться быстродействием « тяжелых » приложений и игр на 5,5-дюймовом дисплее . аппарат получил экран , как у ipad pro , так что картинка теперь соответствует кинематографическому стандарту .'],\n",
       "       ['пион ирис ромашка рассада',\n",
       "        'пион куст 500 р ( более 10 шт)/ \\n      саженец/ корень 100р/ \\n      растут у нас более 70 лет/ \\n      розовые , бордовые и белые/ \\n      на фото цветы 2018г/ \\n      п . зубчаниновка/ \\n      либо пл . революции/ \\n      есть ирисы , ромашка , клубника , боярышник и ирга'],\n",
       "       ['кофта', 'состояние отличное'],\n",
       "       ['1-к квартира , 33 м² , 4/5 эт .',\n",
       "        'продаётся уютная , тёплая квартира в экологически - чистом районе города , рядом сосновый бор , всегда чистый воздух . дом 2004 г . , хорошие соседи , на площадке 2-е квартиры , развитая инфраструктура , в шаговой доступности поликлиника , школа , тк « орбита » , вещевой рынок . квартира в хорошем состоянии . подходит под ипотеку , долгов , обременений , перепланировке нет . в квартире натяжные потолки , в ванной комнате стены выполнены из влагостойких стеновых панелей . возможен обмен на квартиру в г . магнитогорске , торг .'],\n",
       "       ['платье новое 60 размера',\n",
       "        'платье 60 размера , новое , красивого темно синего цвета , из трикотажной ткани : вискоза 95 % , эластина 5 % .а - образного силуэта с рукавом 2/3.длинна по спинке 113 см .'],\n",
       "       ['ваз 2114 samara , 2007',\n",
       "        'продам ваз 2114 2007 г.в . в хорошем состоянии . / \\n      2 владельца , птс оригинал .        / \\n      машина в родной краске , в дтп никогда не была,/ \\n      днище целое не ржавое . по ходовой нареканий нет , сел и поехал . / \\n      имеется музыка , сигнализация 2 комплекта ключей , птф , передние стеклоподъемники./ \\n      небольшой торг при осмотре . / \\n      обмен не интересует .'],\n",
       "       ['наушники блутус',\n",
       "        'долго держат заряд 4 - 5 часов , можно и больше при средней громкости выжать из них . вкладыши .'],\n",
       "       ['пальто tommy hilfiger',\n",
       "        'состояние нового . промахнулась с размером . пальто до -10 -12 градусов . / \\n      возможна пересылка по почте']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 75,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PC8SfKZ53rOE"
   },
   "outputs": [],
   "source": [
    "for i in range(len(X_test)):\n",
    "    for j in range(len(X_test[i])):\n",
    "        X_test[i][j] = preprocess_spacy(X_test[i][j])\n",
    "# токенизовали все предложения тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dmYtpn670YtH",
    "outputId": "57996808-88ba-432d-c4d4-5a71ff138ee1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21000, 2)"
      ]
     },
     "execution_count": 73,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S7T64nKyCpP3"
   },
   "outputs": [],
   "source": [
    "list_of_words = []\n",
    "for i in X_train:\n",
    "    for j in i:\n",
    "        list_of_words += map(str, j.split())\n",
    "counts = Counter(list_of_words)\n",
    "bow_vocabulary2 = counts.most_common(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zmcpdQ-gCsX3"
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in bow_vocabulary2:\n",
    "    result.append(i[0])\n",
    "# получили список самых частовстречающихся слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FCfDQ6lvCxVc"
   },
   "outputs": [],
   "source": [
    "X_train_bow_spacy = items_to_bow(X_train)\n",
    "X_test_bow_spacy = items_to_bow(X_test)\n",
    "# векторизовали каждый объект в выборках методом bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "00ygOM8dDEsC",
    "outputId": "0b0604d3-8df6-46b6-cc86-5074277f5132"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6174444444444445"
      ]
     },
     "execution_count": 87,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_bow_model = LogisticRegression(max_iter=100).fit(X_train_bow_spacy, y_train)\n",
    "accuracy_score(spacy_bow_model.predict(X_test_bow_spacy), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "LKMFagY5DQL6",
    "outputId": "d517de47-3144-4f54-e794-83bdada8f77b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5912222222222222"
      ]
     },
     "execution_count": 88,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_bow_model = LinearSVC(max_iter=70).fit(X_train_bow_spacy, y_train)\n",
    "accuracy_score(spacy_bow_model.predict(X_test_bow_spacy), y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E_3EtGFXTV61"
   },
   "outputs": [],
   "source": [
    "# при таком способе токенизации наблюдается снижение качества. Возможно, что после токенизации в выборке остались ненужные слова или наборы символов. \n",
    "# В каждом примере можно также увеличить количество итераций, но тогда обучение и предсказание будут строиться дольше."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
